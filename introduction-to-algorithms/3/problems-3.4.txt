3-4 Asymptotic notation properties
a. f(n) = O(g(n)) implies g(n) = O(f(n))
Can be proved by contr exapmle n = O(n^2) but not vice versa n^2 = O(n)

b. f(n) + g(n) = theta(min(f(n),g(n))))
c1 * (f(n) + g(n)) <= min(f(n),g(n))) <=c2 * (f(n) + g(n))
suppose that f(n) is the min (they can be used interchangeably)
c1 * (f(n) + g(n)) <= f(n) <=c2 * (f(n) + g(n))
c1 * (1 + g(n)/f(n)) <= 1 <=c2 * (f(n) + g(n))
c1 * (1 + g(n)/f(n)) <= 1  - since g(n) is bigger then f(n) when n goes to infinity
the left part goes to infinity so the conjecture is wrong

c.
1) f(n) = O(g(n)) => f(n) <= c * g(n)
2) lg(f(n)) = O(ln(g(n))) => lg(f(n)) <= c * lg(g(n))

if f(n) is monotonically increasing function, then for x1 >= x2 => f(x1) >= f(x2)
Since lg(n) is monotonically increasing function we can say that
1) implies 2)  (restriction in the book lg(g(n)) >= 1 and f(n) >= 1

d. Similar to c.
Since 2^x is monotonically increasing function (x >= 0)
and we have f(n) = O(g(n)), the 2^(f(n)) = O(2^(f(n)))

e. f(n) = O((f(n))^2)
not true for asymptotically positive functions
f(n) <= c (f(n))^2
1 <= c f(n)
if it's monotonically decreasing function (1/n), for any c f(n) -> 0  to it's not correct.

f. f(n) = O(g(n)) => g(n) = big-omega(f(n))

- f(n) <= c g(n)
- c f(n) <= g(n) == f(n) <= 1/c g(n) == f(n) <= c1 g(n)
which proves the conjecture

g. f(n) = theta (f(n/2))
we can rewrite that as f(2n) = theta(f(n))
c1 f(n) <= f(2n) <= c2 f(n)
c1 <= f(2n)/f(n) <= c2
Using LHospitalsRule we can find the limit as:
lim (f(2n)/f(n)) = lim (f'(2n)/f'(n)) = lim(2 f'(n)/f'(n))= 2
So we can always find the constants c1 and c2 and the conjecture is correct.

h.

